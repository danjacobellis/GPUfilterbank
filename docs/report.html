

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Notebook to PDF (as a document) &mdash; GPU Filter Bank  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/mystnb.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> GPU Filter Bank
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="GPUFilterBanks.html">CUDA Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="dyadic.html">Semi-Constant-Q Dyadic Filter Bank</a></li>
<li class="toctree-l1"><a class="reference internal" href="denoise.html">Denoising Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="timing.html">Timing</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GPU Filter Bank</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Notebook to PDF (as a document)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/report.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="notebook-to-pdf-as-a-document">
<h1>Notebook to PDF (as a document)<a class="headerlink" href="#notebook-to-pdf-as-a-document" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">jupyter-nbconvert</span> <span class="pre">written_report.ipynb</span> <span class="pre">--to</span> <span class="pre">pdf</span> <span class="pre">-TagRemovePreprocessor.remove_cell_tags='{&quot;remove_cell&quot;}'</span> <span class="pre">--template</span> <span class="pre">template.tplx</span></code></p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="notebook-to-slide-deck">
<h1>Notebook to slide deck<a class="headerlink" href="#notebook-to-slide-deck" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">jupyter-nbconvert</span> <span class="pre">presentation.ipynb</span> <span class="pre">--to</span> <span class="pre">slides</span> <span class="pre">--TagRemovePreprocessor.remove_input_tags={\&quot;remove-input\&quot;}</span> <span class="pre">--post</span> <span class="pre">serve</span> <span class="pre">--ServePostProcessor.port=9997</span> <span class="pre">--ServePostProcessor.ip=&quot;0.0.0.0&quot;</span></code></p>
<p>http://72.179.3.141:9997/nov8.slides.html</p>
<p>To convert to pdf, add ?print-pdf to URL, i.e.</p>
<p>http://72.179.3.141:9997/nov8.slides.html?print-pdf</p>
<p>Coverage of topics:</p>
<div class="section" id="dan">
<h2>Dan<a class="headerlink" href="#dan" title="Permalink to this headline">¶</a></h2>
<p>part one:</p>
<ul class="simple">
<li><p>Background</p></li>
<li><p>Time-frequency analysis</p></li>
<li><p>Time - frequency tilings</p></li>
<li><p>Constant-Q</p></li>
<li><p>perfect reconstruction</p></li>
<li><p>Existing transforms and software</p>
<ul>
<li><p>STFT</p></li>
<li><p>mel-filterbanks using FFT</p></li>
<li><p>Discrete wavelet transforms</p></li>
</ul>
</li>
<li><p>common techniques do not have perfect reconstruction</p>
<ul>
<li><p>Either have uniform frequency resolution or perfectly constant-Q</p>
<ul>
<li><p>uniform resolution leads to poor time resolution for high frequencies</p></li>
<li><p>uniform resolution leads to poor frequency resolution for low frequencies</p></li>
<li><p>perfectly constant-Q transforms lead to data structure which is impossible to work with.</p></li>
</ul>
</li>
<li><p>mel filterbanks try to provide a compromise in resolution, but in doing so introduces new problems.</p>
<ul>
<li><p>Undersampled for some frequency bands</p></li>
<li><p>oversampled for others</p></li>
</ul>
</li>
<li><p>FFT based methods require real and imaginary.</p>
<ul>
<li><p>Leads to phase reconstruction problem.</p></li>
<li><p>End up making more work for yourself by having to do griffin-lim or similar algorithm to solve problem which should not have been there in the first place.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Size of data</p></li>
<li><p>Compression and storage</p></li>
</ul>
</div>
<div class="section" id="utsha">
<h2>Utsha<a class="headerlink" href="#utsha" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Haar filter (1min)</p>
<ul>
<li><p>Simplest, dyadic filter</p></li>
<li><p>Perfect reconstruction</p></li>
<li><p>We are using as our building block</p></li>
</ul>
</li>
<li><p>Discussion about GPU &amp; Convolution (2min)</p>
<ul>
<li><p>What is Convolution? (Toeplitz, etc)</p></li>
<li><p>Sequential vs Parallel Convolution</p></li>
<li><p>GPU’s analysis and bottlenecks</p></li>
</ul>
</li>
<li><p>Our algorithm (3min)</p>
<ul>
<li><p>Dyadic Analysis &amp; Sysnthesis Filterbank</p></li>
<li><p>Convolution as a building block</p></li>
<li><p>Decimation &amp; Polyphase filters to utilize convolution</p></li>
<li><p>Cascading unbalanced &amp; balanced binary tree structure</p></li>
</ul>
</li>
<li><p>Complexity analysis (1min)</p></li>
<li><p>Experiment Results (2min)</p></li>
</ul>
</div>
<div class="section" id="id1">
<h2>Dan<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Denoising demo</p></li>
<li><p>Drawbacks of our method</p>
<ul>
<li><p>not shift invariant</p>
<ul>
<li><p>can be fixed by dual-tree complex wavelet transform</p></li>
<li><p>effectively oversamples by two</p></li>
</ul>
</li>
<li><p>We used Haar filters (two point sum and difference).</p>
<ul>
<li><p>Frequency selectivity may not be as good</p></li>
</ul>
</li>
<li><p>Not fully constant-Q</p>
<ul>
<li><p>makes some analysis much easier, but others might be more difficult</p></li>
<li><p>example: music transcription</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="gpu-filter-banks-for-audio">
<h1>GPU Filter Banks for Audio<a class="headerlink" href="#gpu-filter-banks-for-audio" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="abstract">
<h1>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h1>
<p>In our project, we implement a class of perfect-reconstruction filterbanks for audio analysis. We exploit parallelism across CPU threads for part of our algorithm and use a GPU to solve embarassingly parallel subtasks. We also introduce a novel type of time-frequency tiling which retains the benefits of constant-Q transforms without requiring their cumbersome data structure.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="background">
<h1>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h1>
<p>For audio signal processing problems such as compression, denoising, and various types of pattern recognition, deep learning has taken over. For example, the search interest for “wavelet” has followed the opposite trajectory as “convolutional neural network”</p>
<p><img alt="" src="_images/trends1.png" /></p>
<p>A current trend of audio signal processing research is to apply 2D convolutional neural networks to time-frequency representations of audio. Although great progress was made in the pre-deep learning era on perfect reconstruction filterbanks and wavelet transforms, most of the algorithms and techniques have been abandoned by practitioners of deep learning because no standardized and free implementations were ever widely disseminated. As a result, the standarized set of triangular “Mel” filterbanks dating back nearly 50 years appears to have remained the dominant tool, and have even seen a resurgance despite their <a class="reference external" href="https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4">impending obsolescence.</a></p>
<p><img alt="" src="_images/trends2.png" /></p>
<p>The use of these methods is accumulating a considerable debt. Compared to the most recent iterations of constant-Q filterbanks, these methods for time-frequency analysis lack several desirable properties</p>
<ul class="simple">
<li><p>Parameterization/tunability</p></li>
<li><p>Perfect reconstruction</p></li>
<li><p>Meaningful/interpretable phase representation</p></li>
<li><p>Generalization to multiple channels/phased arrays</p></li>
<li><p>Efficient implementation for oversampled representations.</p></li>
</ul>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="filter-banks-and-wavelet-transforms">
<h1>Filter Banks and Wavelet Transforms<a class="headerlink" href="#filter-banks-and-wavelet-transforms" title="Permalink to this headline">¶</a></h1>
<p>Filter banks are the oldest method of time-frequency analysis, dating back to the first analog spectrum analyzer built by Hermann von Helmholtz in the 19th century.</p>
<p><img alt="" src="_images/helmholtz.png" /></p>
<p>In 1909, mathematician Alfréd Haar discovered that a list of <span class="math notranslate nohighlight">\(2^n\)</span> numbers can be represented by recursively taking the sum and difference of adjacent pairs. This property is now commonly referred to ‘alias cancellation’</p>
<p>In the 1970s, engineers created the first discrete, invertible filterbanks by expoiting this property. Using what are known as ‘conjugate mirror filters’. In the following decade, a massive research effort took place, resulting in a rigorous mathematical understanding of these processes and the development of various “wavelet transforms.”</p>
<p>Unfortunately, deep learning has begun to displace these techniques in education, leading to considerable confusion and technical debt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">DSP</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="haar-decomposition">
<h1>Haar Decomposition<a class="headerlink" href="#haar-decomposition" title="Permalink to this headline">¶</a></h1>
<p>(Draft) Haar decomposition is one of the simplest wavelet transform that has perfect reconstruction property, thus is a good introduction example. First of all, note that the input is a sequence of integers, because we are dealing digital data after sampling from raw data, such as audio. By performing two convolutions with the input, we get a low-frequency component and a high-frequency component.
As we can see, the sum and difference of the high and low frequency components forms a splitted signal of the original signal, which we can merge to retain the original one. This is an example of “critically sampled” dyadic filterbank, which means that the size of data after applying the filter is exactly same as original data. Note that, “dyadic filters” refers “dynamic analysis filters”, the type of filtering that consists of a filtering followed by downsampling. There is an inverse operation, which is synthetic filters. (Which we are dealing by convolution. We will get to this detail later)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">round</span><span class="o">.</span><span class="p">(</span><span class="kt">Int</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[7, 1, -13, 20, 4, 7, -18, 5]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">2</span><span class="o">:</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="p">];</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])[</span><span class="mi">2</span><span class="o">:</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="p">];</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">println</span><span class="p">(</span><span class="s">&quot;s = &quot;</span><span class="p">,</span><span class="n">s</span><span class="p">);</span> <span class="n">print</span><span class="p">(</span><span class="s">&quot;d = &quot;</span><span class="p">,</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>s = [8, 7, 11, -13]
d = [-6, 33, 3, 23]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">y1</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="o">-</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span>
<span class="n">y2</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="o">+</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">println</span><span class="p">(</span><span class="kt">Int</span><span class="o">.</span><span class="p">(</span><span class="n">y1</span><span class="p">));</span> <span class="n">print</span><span class="p">(</span><span class="kt">Int</span><span class="o">.</span><span class="p">(</span><span class="n">y2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[7, -13, 4, -18]
[1, 20, 7, 5]
</pre></div>
</div>
</div>
</div>
<p>This is an example of a <strong>critically sampled</strong> dyadic filterbank</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="time-frequency-tiling">
<h1>Time-frequency tiling<a class="headerlink" href="#time-frequency-tiling" title="Permalink to this headline">¶</a></h1>
<p>Decomposing the lower branch produces octave filters</p>
<p>img/dyadic_analysis_filterbank.svg</p>
<p><img alt="" src="_images/octave_tiling.png" /></p>
<p>Decomposing both branches (balanced tree)</p>
<p>(uniform grid of rectangles)</p>
<p>you can have anything in the middle</p>
<p>(Draft)
In the Haar decomposition, we received two signals with half time resolution but twice the frequency resolution signals, by applying the filters. Next, we introduce a more fine-grained time-frequency tiling pattern, as shown in the image.</p>
<p>This structure has the benefit of a simple data structure, since each tile is a vector, while providing the exact trade-off between the frequency resolution and the time resolution as we want.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="dwt-sequential-time-complexity">
<h1>DWT Sequential time complexity<a class="headerlink" href="#dwt-sequential-time-complexity" title="Permalink to this headline">¶</a></h1>
<p>each level is N log N</p>
<p>log N levels</p>
<p>between N log N and N (log N)^2</p>
<p>regardless of how many levels you go down</p>
<p>(Draft) In general, we have between <span class="math notranslate nohighlight">\(N\log N\)</span> and <span class="math notranslate nohighlight">\(N (\log N)^2\)</span> time complexity to perform any discrete wavelet transform, because at each level of filtering tree, it takes <span class="math notranslate nohighlight">\(N \log N\)</span> time and there are <span class="math notranslate nohighlight">\(\log N\)</span> levels.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="parallel-fft-complexity-and-therefore-convolution">
<h1>parallel fft complexity (and therefore convolution)<a class="headerlink" href="#parallel-fft-complexity-and-therefore-convolution" title="Permalink to this headline">¶</a></h1>
<p>log N levels</p>
<p>each level n/p computation</p>
<p>(N/p) log N</p>
<p>parallel DWT is</p>
<p>(N/p) (log N)^2</p>
<p>(Draft) Parallel FFT (Fast Fourier Transform) is widely used in any signal processing application. We can see FFT as a version of filterbank, which breaks the input signal down all the way to full frequency resolution, but with no time resolution.</p>
<p>Similar to general DWT, we can see FFT consisting of <span class="math notranslate nohighlight">\(\log N\)</span> levels. Each level has <span class="math notranslate nohighlight">\(\frac{N}{p}\)</span> computation
, thus time complexity for parallel FFT is <span class="math notranslate nohighlight">\((\frac{N}{p})\log N\)</span>. In contrast, parallel DWT takes <span class="math notranslate nohighlight">\((\frac{N}{p})(\log N)^2\)</span> time.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="input-size">
<h1>Input size<a class="headerlink" href="#input-size" title="Permalink to this headline">¶</a></h1>
<p>Speech: few seconds long, fs = 16000, N ~ 10^5 per record.</p>
<p>Music: minutes long, two channels, fs = 48000, N ~ 10^7 per record.</p>
<p>Passive sonar array: 100s of channels, fs in the kHz, ~ 10^10 per hour.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="data-pipeline">
<h1>Data pipeline<a class="headerlink" href="#data-pipeline" title="Permalink to this headline">¶</a></h1>
<p>Speech and music are nearly always compressed. Typical compression ratio is 10:1</p>
<p>Phased array data often minimally compressed and stored on cheap magnetic tape.</p>
<p>Often helpful to combine PRAM and GPU computation models.</p>
<p>Multiple CPUs can increase the rate at which data is decoded and communicated to the GPU(s)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="generalization-to-other-filters">
<h1>Generalization to other filters<a class="headerlink" href="#generalization-to-other-filters" title="Permalink to this headline">¶</a></h1>
<p>You need alias cancellation</p>
<p>https://en.wikipedia.org/wiki/Quadrature_mirror_filter</p>
<p>As long as you satisfy this condition, you can use better filters</p>
<p>(Draft) The framework to process the input signal by the analysis filterbank and then reconstruct to get it back is generalizable. We can apply any operation on the in-between layer (i.e. the processed small data blocks) and retain the perfect reconstruction property, as long as the operation has mirror-conjugate property.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="discrete-wavelet-transform">
<h1>Discrete wavelet transform<a class="headerlink" href="#discrete-wavelet-transform" title="Permalink to this headline">¶</a></h1>
<p>Generalizes this idea</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="goal">
<h1>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h1>
<p>Generalize these types of transformations to</p>
<ul class="simple">
<li><p>Filters with greater frequency selectivity</p></li>
<li><p>Oversampled filters</p></li>
<li><p>Undersampled filters</p></li>
</ul>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="gpu-convolution">
<h1>GPU convolution<a class="headerlink" href="#gpu-convolution" title="Permalink to this headline">¶</a></h1>
<p>what is CUDA doing?</p>
<p>https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html</p>
<p>If you repeat your data and construct</p>
<p>https://en.wikipedia.org/wiki/Toeplitz_matrix</p>
<p>Then convolution is equivalent to matrix multiply</p>
<p>Not actually doing matrix multiplication, but create a mapping from “virtual matrix” to corresponding array location</p>
<p>(fold in some of the theory from lecture on mat mul)</p>
<p>(Draft) We utilized existing implementation of CuDNN convolution, as it is optimized for GPU as shipped. The convolution on digital signal can be expressed as a multiplication of Toepliz matrix and the convolution kernel - whose rows consists of a shifted element of the original signal (i.e. sequence of numbers).
Note that Toepliz matrix multiplication is much efficient than regular matrix multiplication, because in implementation we do not need to store data in a matrix. Instead, it can be operated with indices and compute convolution, by creating a mapping from “virtual matrix” to the corresponding array location.</p>
<p>convolution is identical to mat vec multiply <span class="math notranslate nohighlight">\(Ax\)</span> where <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(N \times (N-M+\text{pad})\)</span> toeplitz and <span class="math notranslate nohighlight">\(x\)</span> is <span class="math notranslate nohighlight">\((N-M+\text{pad})\times 1\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{pad} \in [0,M]\)</span> is the amount of zero padding</p>
<p><span class="math notranslate nohighlight">\(N\)</span> is signal length</p>
<p><span class="math notranslate nohighlight">\(M\)</span> is filter length</p>
<p>Sequential algorithm for convolution:</p>
<p><span class="math notranslate nohighlight">\((N-M+\text{pad})\)</span> dot products.  Each dot product takes <span class="math notranslate nohighlight">\(M^2\)</span> multiplies  and <span class="math notranslate nohighlight">\(M-1\)</span> additions.</p>
<p><span class="math notranslate nohighlight">\( y = x \ast h = 
\begin{bmatrix}
    x_1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
    x_2 &amp; x_1 &amp;      &amp; \vdots &amp; \vdots \\
    x_3 &amp; x_2 &amp; \cdots &amp; 0 &amp; 0 \\
    \vdots &amp; x_3 &amp; \cdots &amp; x_1 &amp; 0 \\
    x_{n-1} &amp; \vdots &amp; \ddots &amp; x_2 &amp; x_1 \\
    x_n &amp; x_{n-1} &amp;      &amp; \vdots &amp; x_2 \\
    0 &amp; x_n &amp; \ddots &amp; x_{n-2} &amp; \vdots \\
    0 &amp; 0 &amp; \cdots &amp; x_{n-1} &amp; x_{n-2} \\
    \vdots &amp; \vdots &amp;        &amp; x_n &amp; x_{n-1} \\
    0 &amp; 0 &amp; 0 &amp; \cdots &amp; x_n
\end{bmatrix}
\)</span>
<span class="math notranslate nohighlight">\(\begin{bmatrix}
    0 \\
    0 \\
    \vdots\\
    h_1 \\
    h_2 \\
    \vdots \\
    h_m \\
    0 \\
    \vdots \\
    0 \\
\end{bmatrix}\)</span></p>
<div class="section" id="parallel-convolution">
<h2>parallel convolution<a class="headerlink" href="#parallel-convolution" title="Permalink to this headline">¶</a></h2>
<p>Option 1: number of available processors is approximately N:</p>
<p>Use <span class="math notranslate nohighlight">\(p=(N-M+\text{pad})\)</span> processors, Each processor does the dot product sequentially using <span class="math notranslate nohighlight">\(M^2\)</span> multiplies and <span class="math notranslate nohighlight">\(M-1\)</span> additions.</p>
<p>Second option: number of available processors is <span class="math notranslate nohighlight">\(\geq NM\)</span></p>
<p>use <span class="math notranslate nohighlight">\(NM\)</span> processors to perform all <span class="math notranslate nohighlight">\(N\)</span> Hadamard products in parallel.</p>
<p>Use parallel reduce to compute the sum in <span class="math notranslate nohighlight">\(log_2(M)\)</span> steps</p>
<div class="section" id="question-how-many-p-would-we-need-for-audio-signals">
<h3>Question: how many <span class="math notranslate nohighlight">\(p\)</span> would we need for audio signals?<a class="headerlink" href="#question-how-many-p-would-we-need-for-audio-signals" title="Permalink to this headline">¶</a></h3>
<p>Speech: few seconds long, fs = 16000, N ~ 10^5 per record.</p>
<p>Music: minutes long, two channels, fs = 48000, N ~ 10^7 per record.</p>
<p>Passive sonar array: 100s of channels, fs in the kHz, ~ 10^10 per hour.</p>
</div>
<div class="section" id="ideally-we-would-have-10-10-processors">
<h3>Ideally, we would have 10^10 processors.<a class="headerlink" href="#ideally-we-would-have-10-10-processors" title="Permalink to this headline">¶</a></h3>
<p>In practice N is limited by the ability of CPU/ data storage to stream data onto the GPU.</p>
<p>GPU clock rate is typically about 1 Ghz. There is <span class="math notranslate nohighlight">\(\approx 1 \text{ns}\)</span> between clock cycles on GPU. N is chosen based on how much data can be loaded into memory in that period of time.</p>
<p>For small M, convolution is effectively <span class="math notranslate nohighlight">\(O(1)\)</span> on GPU, but is limited by memory transfer rate.</p>
<p>Deep learning practitioners have highly optimized this problem (convolution between large N and small M). We want to build our algorithm on top of their software and highly optimized data pipeline.</p>
<p>Exactly the type of thing that GPUs are built for.</p>
<p>Use gpus which act like <span class="math notranslate nohighlight">\(p = O(10^3)\)</span> processors. Break up data so that <span class="math notranslate nohighlight">\(NM\)</span> words of data can be stored on the gpu.</p>
<p>often bottlenecked by ability of CPU to decode compressed data.</p>
<p>Really just limited by ability of CPU to transfer memory back and forth.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="fundamental-building-block-of-our-algorithm">
<h1>Fundamental building block of our algorithm:<a class="headerlink" href="#fundamental-building-block-of-our-algorithm" title="Permalink to this headline">¶</a></h1>
<p>Highly optimized CuDNN convolution routine which is suited to large N and small M.</p>
<p>If M is small enough, limited by memory transfer.</p>
<p>If M is large, limited by GPU clock rate.</p>
<p>If we go all the way from time domain to frequency domain, binary tree would have depth of <span class="math notranslate nohighlight">\(log N\)</span>. In practice, we stop about halfway through.</p>
<p>This results in roughly equal time and frequency resolution which is what we want for useful signal processing tasks (denoising, compression, statistical learning, etc).</p>
<p>Summary: time complexity of our algorithm is <span class="math notranslate nohighlight">\(O(log N)\)</span>, but in practice is limited by transfer rate since <span class="math notranslate nohighlight">\(M\)</span> is small.</p>
<div class="section" id="parallel-dot-product">
<h2>parallel dot product<a class="headerlink" href="#parallel-dot-product" title="Permalink to this headline">¶</a></h2>
<p>product part can also be computed in parallel trivially</p>
<p>sum is just parallel reduce.</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="decimation-filterbank">
<h1>Decimation filterbank<a class="headerlink" href="#decimation-filterbank" title="Permalink to this headline">¶</a></h1>
<p>filter then downsample</p>
<p>!= dilation + stride (but close)</p>
<p>show toeplitz matrix multiplication with upsampling/downsampling</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="polyphase-filterbank">
<h1>Polyphase filterbank<a class="headerlink" href="#polyphase-filterbank" title="Permalink to this headline">¶</a></h1>
<p>http://users.ece.utexas.edu/~bevans/courses/realtime/lectures/13_Digital_PAM/lecture13.ppt</p>
<p>upsample then filter</p>
<p>?= transposed convolution with dilation and stride</p>
<p>Another oppurtunity to add CPU parallelism</p>
<p>show mathematically that it’s equivalent to splitting filter into L pieces where L is the upsampling factor, and applying filters in parallel</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="oversampled-filterbanks">
<h1>Oversampled filterbanks<a class="headerlink" href="#oversampled-filterbanks" title="Permalink to this headline">¶</a></h1>
<p>(Draft) Oversampled filterbanks refers to the type of filterbank that results in larger sized output data compared to the input. For example, Fourier Transform is an oversampled filtering with factor 2, as it yields a signal in frequency domain and the phase domain.</p>
<p>Because of this nature, oversampled filterbank can get better CPU parallelism by having multiple data pipeline.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="undersampled-filterbanks">
<h1>Undersampled filterbanks<a class="headerlink" href="#undersampled-filterbanks" title="Permalink to this headline">¶</a></h1>
<p>(Draft) In contrast to oversampled filterbanks, undersampled filterbank results in smaller sized output data compared to the input data. An example is convolving input signal with a small kernal without paddings.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="analysis-filterbank">
<h1>Analysis filterbank<a class="headerlink" href="#analysis-filterbank" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="synthesis-filterbank">
<h1>Synthesis filterbank<a class="headerlink" href="#synthesis-filterbank" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="processing">
<h1>Processing<a class="headerlink" href="#processing" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>denoising</p></li>
<li><p>compression</p></li>
<li><p>source separation</p></li>
</ul>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="octave-filters-balanced-filters">
<h1>Octave Filters + Balanced Filters<a class="headerlink" href="#octave-filters-balanced-filters" title="Permalink to this headline">¶</a></h1>
<p>In our filter, we repeatedly apply analysis filters to break down the input signal and synthesis filters to reconstruct from broken down (and possibly processed) pieces. As shown in the earlier image, this has an unbalanced tree structure where we apply analysis filters to the higher frequency-band output at each level, but leave the lower frequency-band output. This is a natural structure, as human ear processes sound signals in a similar way.</p>
<p>In our algorithm, we break down the signal into 11 octaves, as 11 is mostly sufficient to cover human’s hearing capability. As a result, we get 11 vectors, where higher frequency blocks have better resolution.  Note that, we can apply any operation on these vectors and still retain the reconstruction property, as long as the operation has the mirror-conjugate property.</p>
<p>Following the octave-spaced analysis filter, we apply dyadic filters in a balanced-tree fashion (i.e. apply filters to both high and low frequency outputs), to obtain some fixed frequency-resolution blocks out of the imbalanced length blocks from the previous step.</p>
<p>Now, we have obtained 11 blocks of signals, decomposed from the orignal signal, where each block has the exact same frequency resolutions. We can see this as a feature representatino of the original data, where the features size is exactly same as original (i.e. critially sampled).</p>
<p>Later, we will demonstrate the ease of use with this decomposition. We can perform various useful operations such as denoising and comporession with very simple code, and it is highly efficient due to massive parallelism.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="demo-our-program">
<h1>Demo our program<a class="headerlink" href="#demo-our-program" title="Permalink to this headline">¶</a></h1>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>